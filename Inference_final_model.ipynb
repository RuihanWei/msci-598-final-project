{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDZb7QCOfwkF"
      },
      "outputs": [],
      "source": [
        "!pip install TensorFlow==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KDi6gGpKHYa",
        "outputId": "e4b9ebf1-69bd-4536-9e19-f4de3de9d7b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot in c:\\users\\tobyw\\anaconda3\\lib\\site-packages (1.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\tobyw\\anaconda3\\lib\\site-packages (from pydot) (3.0.4)\n",
            "Requirement already satisfied: pydotplus in c:\\users\\tobyw\\anaconda3\\lib\\site-packages (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\tobyw\\anaconda3\\lib\\site-packages (from pydotplus) (3.0.4)\n",
            "Collecting graphviz\n",
            "  Downloading graphviz-0.19.1-py3-none-any.whl (46 kB)\n",
            "Installing collected packages: graphviz\n",
            "Successfully installed graphviz-0.19.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydot\n",
        "!pip install pydotplus\n",
        "!pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiyWWVQKGcLf",
        "outputId": "6a423f86-136d-43d6-8a14-52a54afe8667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9WV24UVDLDu",
        "outputId": "31d0c790-50f0-4245-fad2-634f4c84d0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md1QdsZ37DzY"
      },
      "source": [
        "# utils and preprocessing\n",
        "##### credit: In the following sections of code, some preprocessing and util code were adapted from fnc-1-baseline (https://github.com/FakeNewsChallenge/fnc-1-baseline) and UCL Machine Reading - FNC-1 Submission (https://github.com/uclnlp/fakenewschallenge). Specifically, funcitons to generate the features  word overlap, refuting, polarity, and co-occurrences were adopted from the baseline code. Functions and classes to preprocess and generate TF vectors and cos similiarity are adopted from UCL Machine Reading - FNC-1 Submission. However, both are modifed to serve our design. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OrwFz6Gt9h2d"
      },
      "outputs": [],
      "source": [
        "##### credit: In the following sections of code, some preprocessing and util code were adapted from fnc-1-baseline (https://github.com/FakeNewsChallenge/fnc-1-baseline) and UCL Machine Reading - FNC-1 Submission (https://github.com/uclnlp/fakenewschallenge). Specifically, funcitons to generate the features  word overlap, refuting, polarity, and co-occurrences were adopted from the baseline code. Functions and classes to preprocess and generate TF vectors and cos similiarity are adopted from UCL Machine Reading - FNC-1 Submission. However, both are modifed to serve our design. \n",
        "\n",
        "# the following code is from baseline\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn import feature_extraction\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def normalize_word(w):\n",
        "    return _wnl.lemmatize(w).lower()\n",
        "\n",
        "\n",
        "def get_tokenized_lemmas(s):\n",
        "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "\n",
        "def clean(s):\n",
        "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
        "\n",
        "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
        "\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    # Removes stopwords from a list of tokens\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "\n",
        "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n",
        "    if not os.path.isfile(feature_file):\n",
        "        feats = feat_fn(headlines, bodies)\n",
        "        np.save(feature_file, feats)\n",
        "\n",
        "    return np.load(feature_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def word_overlap_features(headlines, bodies):\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        clean_headline = clean(headline)\n",
        "        clean_body = clean(body)\n",
        "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
        "        clean_body = get_tokenized_lemmas(clean_body)\n",
        "        features = [\n",
        "            len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]\n",
        "        X.append(features)\n",
        "    return X\n",
        "\n",
        "\n",
        "def refuting_features(headlines, bodies):\n",
        "    _refuting_words = [\n",
        "        'fake',\n",
        "        'fraud',\n",
        "        'hoax',\n",
        "        'false',\n",
        "        'deny', 'denies',\n",
        "        # 'refute',\n",
        "        'not',\n",
        "        'despite',\n",
        "        'nope',\n",
        "        'doubt', 'doubts',\n",
        "        'bogus',\n",
        "        'debunk',\n",
        "        'pranks',\n",
        "        'retract'\n",
        "    ]\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        clean_headline = clean(headline)\n",
        "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
        "        features = [1 if word in clean_headline else 0 for word in _refuting_words]\n",
        "        X.append(features)\n",
        "    return X\n",
        "\n",
        "\n",
        "def polarity_features(headlines, bodies):\n",
        "    _refuting_words = [\n",
        "        'fake',\n",
        "        'fraud',\n",
        "        'hoax',\n",
        "        'false',\n",
        "        'deny', 'denies',\n",
        "        'not',\n",
        "        'despite',\n",
        "        'nope',\n",
        "        'doubt', 'doubts',\n",
        "        'bogus',\n",
        "        'debunk',\n",
        "        'pranks',\n",
        "        'retract'\n",
        "    ]\n",
        "\n",
        "    def calculate_polarity(text):\n",
        "        tokens = get_tokenized_lemmas(text)\n",
        "        return sum([t in _refuting_words for t in tokens]) % 2\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        clean_headline = clean(headline)\n",
        "        clean_body = clean(body)\n",
        "        features = []\n",
        "        features.append(calculate_polarity(clean_headline))\n",
        "        features.append(calculate_polarity(clean_body))\n",
        "        X.append(features)\n",
        "    return np.array(X)\n",
        "\n",
        "\n",
        "def ngrams(input, n):\n",
        "    input = input.split(' ')\n",
        "    output = []\n",
        "    for i in range(len(input) - n + 1):\n",
        "        output.append(input[i:i + n])\n",
        "    return output\n",
        "\n",
        "\n",
        "def chargrams(input, n):\n",
        "    output = []\n",
        "    for i in range(len(input) - n + 1):\n",
        "        output.append(input[i:i + n])\n",
        "    return output\n",
        "\n",
        "\n",
        "def append_chargrams(features, text_headline, text_body, size):\n",
        "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
        "    grams_hits = 0\n",
        "    grams_early_hits = 0\n",
        "    grams_first_hits = 0\n",
        "    for gram in grams:\n",
        "        if gram in text_body:\n",
        "            grams_hits += 1\n",
        "        if gram in text_body[:255]:\n",
        "            grams_early_hits += 1\n",
        "        if gram in text_body[:100]:\n",
        "            grams_first_hits += 1\n",
        "    features.append(grams_hits)\n",
        "    features.append(grams_early_hits)\n",
        "    features.append(grams_first_hits)\n",
        "    return features\n",
        "\n",
        "\n",
        "def append_ngrams(features, text_headline, text_body, size):\n",
        "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
        "    grams_hits = 0\n",
        "    grams_early_hits = 0\n",
        "    for gram in grams:\n",
        "        if gram in text_body:\n",
        "            grams_hits += 1\n",
        "        if gram in text_body[:255]:\n",
        "            grams_early_hits += 1\n",
        "    features.append(grams_hits)\n",
        "    features.append(grams_early_hits)\n",
        "    return features\n",
        "\n",
        "\n",
        "def hand_features(headlines, bodies):\n",
        "\n",
        "    def binary_co_occurence(headline, body):\n",
        "        # Count how many times a token in the title\n",
        "        # appears in the body text.\n",
        "        bin_count = 0\n",
        "        bin_count_early = 0\n",
        "        for headline_token in clean(headline).split(\" \"):\n",
        "            if headline_token in clean(body):\n",
        "                bin_count += 1\n",
        "            if headline_token in clean(body)[:255]:\n",
        "                bin_count_early += 1\n",
        "        return [bin_count, bin_count_early]\n",
        "\n",
        "    def binary_co_occurence_stops(headline, body):\n",
        "        # Count how many times a token in the title\n",
        "        # appears in the body text. Stopwords in the title\n",
        "        # are ignored.\n",
        "        bin_count = 0\n",
        "        bin_count_early = 0\n",
        "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
        "            if headline_token in clean(body):\n",
        "                bin_count += 1\n",
        "                bin_count_early += 1\n",
        "        return [bin_count, bin_count_early]\n",
        "\n",
        "    def count_grams(headline, body):\n",
        "        # Count how many times an n-gram of the title\n",
        "        # appears in the entire body, and intro paragraph\n",
        "\n",
        "        clean_body = clean(body)\n",
        "        clean_headline = clean(headline)\n",
        "        features = []\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
        "        return features\n",
        "\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        X.append(binary_co_occurence(headline, body)\n",
        "                 + binary_co_occurence_stops(headline, body)\n",
        "                 + count_grams(headline, body))\n",
        "\n",
        "\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6TRfHD1z7F9_"
      },
      "outputs": [],
      "source": [
        "##### credit: In the following sections of code, some preprocessing and util code were adapted from fnc-1-baseline (https://github.com/FakeNewsChallenge/fnc-1-baseline) and UCL Machine Reading - FNC-1 Submission (https://github.com/uclnlp/fakenewschallenge). Specifically, funcitons to generate the features  word overlap, refuting, polarity, and co-occurrences were adopted from the baseline code. Functions and classes to preprocess and generate TF vectors and cos similiarity are adopted from UCL Machine Reading - FNC-1 Submission. However, both are modifed to serve our design. \n",
        "\n",
        "# the following code is adopted and modified from UCL Machine Reading - FNC-1 Submission (https://github.com/uclnlp/fakenewschallenge)\n",
        "\n",
        "\n",
        "# Copyright 2017 Benjamin Riedel\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "# Import relevant packages and modules\n",
        "from csv import DictReader\n",
        "from csv import DictWriter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Initialise global variables\n",
        "label_ref = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
        "label_ref_keras = {'agree': [1,0,0,0], 'disagree': [0,1,0,0], 'discuss': [0,0,1,0], 'unrelated': [0,0,0,1]}\n",
        "label_ref_rev = {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}\n",
        "stop_words = [\n",
        "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
        "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
        "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
        "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
        "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
        "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
        "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
        "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
        "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
        "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
        "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
        "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
        "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
        "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
        "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
        "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
        "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
        "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
        "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
        "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
        "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
        "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
        "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
        "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
        "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
        "        ]\n",
        "\n",
        "def score_submission(gold_labels, test_labels):\n",
        "    score = 0.0\n",
        "    cm = [[0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0]]\n",
        "\n",
        "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
        "        g_stance, t_stance = g, t\n",
        "        if g_stance == t_stance:\n",
        "            score += 0.25\n",
        "            if g_stance != 'unrelated':\n",
        "                score += 0.50\n",
        "        if g_stance in RELATED and t_stance in RELATED:\n",
        "            score += 0.25\n",
        "\n",
        "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
        "\n",
        "    return score, cm\n",
        "\n",
        "\n",
        "def print_confusion_matrix(cm):\n",
        "    lines = []\n",
        "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
        "    line_len = len(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "    lines.append(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "\n",
        "    hit = 0\n",
        "    total = 0\n",
        "    for i, row in enumerate(cm):\n",
        "        hit += row[i]\n",
        "        total += sum(row)\n",
        "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
        "                                                                   *row))\n",
        "        lines.append(\"-\"*line_len)\n",
        "    print('\\n'.join(lines))\n",
        "\n",
        "\n",
        "def report_score(actual,predicted):\n",
        "    score,cm = score_submission(actual,predicted)\n",
        "    best_score, _ = score_submission(actual,actual)\n",
        "\n",
        "    print_confusion_matrix(cm)\n",
        "    print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(score*100/best_score) + \"%)\")\n",
        "    return score*100/best_score\n",
        "\n",
        "# Define data class\n",
        "class FNCData:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Define class for Fake News Challenge data\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, instances_input, bodies_input):\n",
        "\n",
        "        # Load data\n",
        "        self.instances = instances_input\n",
        "        bodies = bodies_input\n",
        "        self.heads = {}\n",
        "        self.bodies = {}\n",
        "\n",
        "        # Process instances\n",
        "        for instance in self.instances:\n",
        "            if instance['Headline'] not in self.heads:\n",
        "                head_id = len(self.heads)\n",
        "                self.heads[instance['Headline']] = head_id\n",
        "            instance['Body ID'] = int(instance['Body ID'])\n",
        "\n",
        "        # Process bodies\n",
        "        for body in bodies:\n",
        "            self.bodies[int(body['Body ID'])] = body['articleBody']\n",
        "\n",
        "    def read(self, filename):\n",
        "\n",
        "        \"\"\"\n",
        "        Read Fake News Challenge data from CSV file\n",
        "\n",
        "        Args:\n",
        "            filename: str, filename + extension\n",
        "\n",
        "        Returns:\n",
        "            rows: list, of dict per instance\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialise\n",
        "        rows = []\n",
        "\n",
        "        # Process file\n",
        "        with open(filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "\n",
        "        return rows\n",
        "\n",
        "\n",
        "# Define relevant functions\n",
        "def pipeline_train(train, test, lim_unigram):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Process train set, create relevant vectorizers\n",
        "\n",
        "    Args:\n",
        "        train: FNCData object, train set\n",
        "        test: FNCData object, test set\n",
        "        lim_unigram: int, number of most frequent words to consider\n",
        "\n",
        "    Returns:\n",
        "        train_set: list, of numpy arrays\n",
        "        train_stances: list, of ints\n",
        "        bow_vectorizer: sklearn CountVectorizer\n",
        "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
        "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise\n",
        "    heads = []\n",
        "    heads_track = {}\n",
        "    bodies = []\n",
        "    bodies_track = {}\n",
        "    body_ids = []\n",
        "    id_ref = {}\n",
        "    train_set = []\n",
        "    train_stances = []\n",
        "    cos_track = {}\n",
        "    test_heads = []\n",
        "    test_heads_track = {}\n",
        "    test_bodies = []\n",
        "    test_bodies_track = {}\n",
        "    test_body_ids = []\n",
        "    head_tfidf_track = {}\n",
        "    body_tfidf_track = {}\n",
        "    tfidf_cos_ls = []\n",
        "\n",
        "    # Identify unique heads and bodies\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            heads.append(head)\n",
        "            heads_track[head] = 1\n",
        "        if body_id not in bodies_track:\n",
        "            bodies.append(train.bodies[body_id])\n",
        "            bodies_track[body_id] = 1\n",
        "            body_ids.append(body_id)\n",
        "\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in test_heads_track:\n",
        "            test_heads.append(head)\n",
        "            test_heads_track[head] = 1\n",
        "        if body_id not in test_bodies_track:\n",
        "            test_bodies.append(test.bodies[body_id])\n",
        "            test_bodies_track[body_id] = 1\n",
        "            test_body_ids.append(body_id)\n",
        "\n",
        "    # Create reference dictionary\n",
        "    for i, elem in enumerate(heads + body_ids):\n",
        "        id_ref[elem] = i\n",
        "\n",
        "    # Create vectorizers and BOW and TF arrays for train set\n",
        "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
        "    bow = bow_vectorizer.fit_transform(heads + bodies)  # Train set only\n",
        "\n",
        "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
        "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
        "        fit(heads + bodies + test_heads + test_bodies)  # Train and test sets\n",
        "\n",
        "    # Process train set\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        head_tf = tfreq[id_ref[head]].reshape(1, -1)\n",
        "        body_tf = tfreq[id_ref[body_id]].reshape(1, -1)\n",
        "        if head not in head_tfidf_track:\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray()\n",
        "            head_tfidf_track[head] = head_tfidf\n",
        "        else:\n",
        "            head_tfidf = head_tfidf_track[head]\n",
        "        if body_id not in body_tfidf_track:\n",
        "            body_tfidf = tfidf_vectorizer.transform([train.bodies[body_id]]).toarray()\n",
        "            body_tfidf_track[body_id] = body_tfidf\n",
        "        else:\n",
        "            body_tfidf = body_tfidf_track[body_id]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        # feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf])\n",
        "        tfidf_cos_ls.append(tfidf_cos)\n",
        "        train_set.append(feat_vec)\n",
        "        # train_stances.append(label_ref[instance['Stance']])\n",
        "        train_stances.append(label_ref_keras[instance['Stance']])\n",
        "\n",
        "    return train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, tfidf_cos_ls\n",
        "\n",
        "\n",
        "def pipeline_test_with_stance(test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer):\n",
        "\n",
        "    # Initialise\n",
        "    test_set = []\n",
        "    test_stances = []\n",
        "    heads_track = {}\n",
        "    bodies_track = {}\n",
        "    cos_track = {}\n",
        "    test_cos = []\n",
        "\n",
        "    # Process test set\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            head_bow = bow_vectorizer.transform([head]).toarray()\n",
        "            head_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray().reshape(1, -1)\n",
        "            heads_track[head] = (head_tf, head_tfidf)\n",
        "        else:\n",
        "            head_tf = heads_track[head][0]\n",
        "            head_tfidf = heads_track[head][1]\n",
        "        if body_id not in bodies_track:\n",
        "            body_bow = bow_vectorizer.transform([test.bodies[body_id]]).toarray()\n",
        "            body_tf = tfreq_vectorizer.transform(body_bow).toarray()[0].reshape(1, -1)\n",
        "            body_tfidf = tfidf_vectorizer.transform([test.bodies[body_id]]).toarray().reshape(1, -1)\n",
        "            bodies_track[body_id] = (body_tf, body_tfidf)\n",
        "        else:\n",
        "            body_tf = bodies_track[body_id][0]\n",
        "            body_tfidf = bodies_track[body_id][1]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        # feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf])\n",
        "        test_cos.append(tfidf_cos)\n",
        "        test_set.append(feat_vec)\n",
        "        # test_stances.append(label_ref[instance['Stance']])\n",
        "        test_stances.append(label_ref_keras[instance['Stance']])\n",
        "\n",
        "\n",
        "    return test_set, test_stances, test_cos\n",
        "\n",
        "def load_model(sess):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Load TensorFlow model\n",
        "\n",
        "    Args:\n",
        "        sess: TensorFlow session\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, './model/model.checkpoint')\n",
        "\n",
        "\n",
        "def save_predictions(pred, file):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Save predictions to CSV file\n",
        "\n",
        "    Args:\n",
        "        pred: numpy array, of numeric predictions\n",
        "        file: str, filename + extension\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file, 'w') as csvfile:\n",
        "        fieldnames = ['Stance']\n",
        "        writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for instance in pred:\n",
        "            writer.writerow({'Stance': label_ref_rev[instance]})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Features\n",
        "\n",
        "#### credit: TF Vector and TF cos similiarity features are adpated from UCL Machine Reading - FNC-1 Submission (https://github.com/uclnlp/fakenewschallenge)  "
      ],
      "metadata": {
        "id": "cW2KTvjb7Es_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "42WlytrTN-tE"
      },
      "outputs": [],
      "source": [
        "##### credit: In the following sections of code, some preprocessing and util code were adapted from fnc-1-baseline (https://github.com/FakeNewsChallenge/fnc-1-baseline) and UCL Machine Reading - FNC-1 Submission (https://github.com/uclnlp/fakenewschallenge). Specifically, funcitons to generate the features  word overlap, refuting, polarity, and co-occurrences were adopted from the baseline code. Functions and classes to preprocess and generate TF vectors and cos similiarity are adopted from UCL Machine Reading - FNC-1 Submission. However, both are modifed to serve our design. \n",
        "\n",
        "# the following code is adopted and modified from UCL Machine Reading - FNC-1 Submission (https://github.com/uclnlp/fakenewschallenge)\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_VOCAB_SIZE = 5000\n",
        "local = False\n",
        "\n",
        "if local:\n",
        "    EMB_DIR = \"data/glove.6B.50d.txt\"\n",
        "    FEATURES_DIR = \"features\"\n",
        "else:\n",
        "    EMB_DIR = \"drive/MyDrive/4B/MSCI 598/Project/data/glove.6B.50d.txt\"\n",
        "    FEATURES_DIR = \"drive/MyDrive/4B/MSCI 598/Project/features\"\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "def read(filename):\n",
        "    rows = []\n",
        "    # Process file\n",
        "    with open(filename, \"r\", encoding='utf-8') as table:\n",
        "        r = DictReader(table)\n",
        "        for line in r:\n",
        "            rows.append(line)\n",
        "\n",
        "    return rows\n",
        "\n",
        "def preprocess(train, test):\n",
        "\n",
        "  lim_unigram = 5000\n",
        "\n",
        "  train_h = []\n",
        "  train_b = []\n",
        "  train_s = []\n",
        "  test_h = []\n",
        "  test_b = []\n",
        "  test_s = []\n",
        "  all = []\n",
        "\n",
        "  heads = []\n",
        "  heads_track = {}\n",
        "  bodies = []\n",
        "  bodies_track = {}\n",
        "  body_ids = []\n",
        "  id_ref = {}\n",
        "  train_tf = []\n",
        "  train_stances = []\n",
        "  cos_track = {}\n",
        "  test_heads_track = {}\n",
        "  test_tf = []\n",
        "  test_heads = []\n",
        "  test_bodies = []\n",
        "  test_bodies_track = {}\n",
        "  test_body_ids = []\n",
        "  head_tfidf_track = {}\n",
        "  body_tfidf_track = {}\n",
        "  train_cos = []\n",
        "  test_cos = []\n",
        "\n",
        "  train_heads = []\n",
        "  train_bodies = []\n",
        "  test_heads = []\n",
        "  test_bodies = []\n",
        "\n",
        "  \n",
        "  max_sent_len = 0\n",
        "  for instance in train.instances:\n",
        "\n",
        "    # Identify unique heads and bodies\n",
        "    head = instance['Headline']\n",
        "    body_id = instance['Body ID']\n",
        "    if head not in heads_track:\n",
        "        heads.append(head)\n",
        "        heads_track[head] = 1\n",
        "    if body_id not in bodies_track:\n",
        "        bodies.append(train.bodies[body_id])\n",
        "        bodies_track[body_id] = 1\n",
        "        body_ids.append(body_id)\n",
        "  \n",
        "  for instance in test.instances:\n",
        "\n",
        "    head = instance['Headline']\n",
        "    body_id = instance['Body ID']\n",
        "    if head not in test_heads_track:\n",
        "        test_heads.append(head)\n",
        "        test_heads_track[head] = 1\n",
        "    if body_id not in test_bodies_track:\n",
        "        test_bodies.append(test.bodies[body_id])\n",
        "        test_bodies_track[body_id] = 1\n",
        "        test_body_ids.append(body_id)\n",
        "\n",
        "  # Create reference dictionary\n",
        "  for i, elem in enumerate(heads + body_ids):\n",
        "      id_ref[elem] = i\n",
        "\n",
        "  # Create vectorizers and BOW and TF arrays for train set\n",
        "  bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
        "  bow = bow_vectorizer.fit_transform(heads + bodies)  # Train set only\n",
        "\n",
        "  tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
        "  tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
        "    fit(heads + bodies + test_heads + test_bodies)  # Train and test sets\n",
        "  \n",
        "  test_heads = []\n",
        "  test_bodies = []\n",
        "\n",
        "  for instance in train.instances:\n",
        "    all.append(instance['Headline'])\n",
        "    all.append(train.bodies[instance['Body ID']])\n",
        "\n",
        "    train_h.append(instance['Headline'])\n",
        "    train_b.append(train.bodies[instance['Body ID']])\n",
        "    train_s.append(label_ref_keras[instance['Stance']])\n",
        "\n",
        "    train_heads.append(instance['Headline'])\n",
        "    train_bodies.append(train.bodies[instance['Body ID']])\n",
        "\n",
        "    max_sent_len = max(max_sent_len, len(instance['Headline'].split()))\n",
        "    max_sent_len = max(max_sent_len, len(train.bodies[instance['Body ID']].split()))\n",
        "\n",
        "    head = instance['Headline']\n",
        "    body_id = instance['Body ID']\n",
        "    head_tf = tfreq[id_ref[head]].reshape(1, -1)\n",
        "    body_tf = tfreq[id_ref[body_id]].reshape(1, -1)\n",
        "    if head not in head_tfidf_track:\n",
        "        head_tfidf = tfidf_vectorizer.transform([head]).toarray()\n",
        "        head_tfidf_track[head] = head_tfidf\n",
        "    else:\n",
        "        head_tfidf = head_tfidf_track[head]\n",
        "    if body_id not in body_tfidf_track:\n",
        "        body_tfidf = tfidf_vectorizer.transform([train.bodies[body_id]]).toarray()\n",
        "        body_tfidf_track[body_id] = body_tfidf\n",
        "    else:\n",
        "        body_tfidf = body_tfidf_track[body_id]\n",
        "    if (head, body_id) not in cos_track:\n",
        "        tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "        cos_track[(head, body_id)] = tfidf_cos\n",
        "    else:\n",
        "        tfidf_cos = cos_track[(head, body_id)]\n",
        "    # feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "    feat_vec = np.squeeze(np.c_[head_tf, body_tf])\n",
        "    train_cos.append(tfidf_cos)\n",
        "    train_tf.append(feat_vec)\n",
        "    # train_stances.append(label_ref[instance['Stance']])\n",
        "    # train_stances.append(label_ref_keras[instance['Stance']])\n",
        "  \n",
        "  # Initialise\n",
        "  test_set = []\n",
        "  test_stances = []\n",
        "  heads_track = {}\n",
        "  bodies_track = {}\n",
        "  cos_track = {}\n",
        "  test_cos = []\n",
        "\n",
        "  # Process test set\n",
        "  for instance in test.instances:\n",
        "    all.append(instance['Headline'])\n",
        "    all.append(test.bodies[instance['Body ID']])\n",
        "\n",
        "    test_h.append(instance['Headline'])\n",
        "    test_b.append(test.bodies[instance['Body ID']])\n",
        "    test_s.append(label_ref_keras[instance['Stance']])\n",
        "\n",
        "    test_heads.append(instance['Headline'])\n",
        "    test_bodies.append(test.bodies[instance['Body ID']])\n",
        "\n",
        "    max_sent_len = max(max_sent_len, len(instance['Headline'].split()))\n",
        "    max_sent_len = max(max_sent_len, len(test.bodies[instance['Body ID']].split()))\n",
        "\n",
        "    head = instance['Headline']\n",
        "    body_id = instance['Body ID']\n",
        "    if head not in heads_track:\n",
        "        head_bow = bow_vectorizer.transform([head]).toarray()\n",
        "        head_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
        "        head_tfidf = tfidf_vectorizer.transform([head]).toarray().reshape(1, -1)\n",
        "        heads_track[head] = (head_tf, head_tfidf)\n",
        "    else:\n",
        "        head_tf = heads_track[head][0]\n",
        "        head_tfidf = heads_track[head][1]\n",
        "    if body_id not in bodies_track:\n",
        "        body_bow = bow_vectorizer.transform([test.bodies[body_id]]).toarray()\n",
        "        body_tf = tfreq_vectorizer.transform(body_bow).toarray()[0].reshape(1, -1)\n",
        "        body_tfidf = tfidf_vectorizer.transform([test.bodies[body_id]]).toarray().reshape(1, -1)\n",
        "        bodies_track[body_id] = (body_tf, body_tfidf)\n",
        "    else:\n",
        "        body_tf = bodies_track[body_id][0]\n",
        "        body_tfidf = bodies_track[body_id][1]\n",
        "    if (head, body_id) not in cos_track:\n",
        "        tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "        cos_track[(head, body_id)] = tfidf_cos\n",
        "    else:\n",
        "        tfidf_cos = cos_track[(head, body_id)]\n",
        "    # feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "    feat_vec = np.squeeze(np.c_[head_tf, body_tf])\n",
        "    test_cos.append(tfidf_cos)\n",
        "    test_tf.append(feat_vec)\n",
        "    # test_stances.append(label_ref[instance['Stance']])\n",
        "    # test_stances.append(label_ref_keras[instance['Stance']])\n",
        "\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n') \n",
        "  tokenizer.fit_on_texts(all)\n",
        "\n",
        "  word_index =  word_index = {k: v for k, v in tokenizer.word_index.items() if v < MAX_VOCAB_SIZE}\n",
        "  idx_to_word = dict((v,k) for k,v in word_index.items())\n",
        "\n",
        "  print(\"heads h test.instances len\", len(test_heads), len(test_h), len(test.instances))\n",
        "\n",
        "  train_h = tokenizer.texts_to_sequences(train_h)\n",
        "  train_h = pad_sequences(train_h, maxlen=max_sent_len, padding='post', truncating='post')\n",
        "\n",
        "  train_b = tokenizer.texts_to_sequences(train_b)\n",
        "  train_b = pad_sequences(train_b, maxlen=max_sent_len, padding='post', truncating='post')\n",
        "\n",
        "  test_h = tokenizer.texts_to_sequences(test_h)\n",
        "  test_h = pad_sequences(test_h, maxlen=max_sent_len, padding='post', truncating='post')\n",
        "\n",
        "  test_b = tokenizer.texts_to_sequences(test_b)\n",
        "  test_b = pad_sequences(test_b, maxlen=max_sent_len, padding='post', truncating='post')\n",
        "  \n",
        "    \n",
        "  train_word_overlap = gen_or_load_feats(word_overlap_features, train_heads, train_bodies, FEATURES_DIR + \"/train/overlap.\"+\"overlap\"+\".npy\")\n",
        "  train_refuting = gen_or_load_feats(refuting_features, train_heads, train_bodies, FEATURES_DIR + \"/train/refuting.\"+\"refuting\"+\".npy\")\n",
        "  train_polarity = gen_or_load_feats(polarity_features, train_heads, train_bodies, FEATURES_DIR + \"/train/polarity.\"+\"polarity\"+\".npy\")\n",
        "  train_hand = gen_or_load_feats(hand_features, train_heads, train_bodies, FEATURES_DIR + \"/train/hand.\"+\"hand\"+\".npy\")\n",
        "  \n",
        "  test_word_overlap = gen_or_load_feats(word_overlap_features, test_heads, test_bodies, FEATURES_DIR + \"/test/overlap.\"+\"overlap\"+\".npy\")\n",
        "  test_refuting = gen_or_load_feats(refuting_features, test_heads, test_bodies, FEATURES_DIR + \"/test/refuting.\"+\"refuting\"+\".npy\")\n",
        "  test_polarity = gen_or_load_feats(polarity_features, test_heads, test_bodies, FEATURES_DIR + \"/test/polarity.\"+\"polarity\"+\".npy\")\n",
        "  test_hand = gen_or_load_feats(hand_features, test_heads, test_bodies, FEATURES_DIR + \"/test/hand.\"+\"hand\"+\".npy\")\n",
        "\n",
        "  print(\"heads h feat len\", len(test_heads), len(test_h), len(test_hand))\n",
        "\n",
        "  # Data path\n",
        "  embeddings = {}\n",
        "  f = open(EMB_DIR, encoding=\"utf8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings[word] = vector\n",
        "  f.close()\n",
        "\n",
        "  embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
        "\n",
        "  for word, i in word_index.items(): # i=0 is the embedding for the zero padding\n",
        "      try:\n",
        "          embeddings_vector = embeddings[word]\n",
        "      except KeyError:\n",
        "          embeddings_vector = None\n",
        "      if embeddings_vector is not None:\n",
        "          embeddings_matrix[i] = embeddings_vector\n",
        "          \n",
        "  del embeddings\n",
        "\n",
        "\n",
        "  return embeddings_matrix, train_h, train_b, train_s, \\\n",
        "  test_h, test_b, test_s, max_sent_len, train_tf, train_cos, \\\n",
        "  test_tf, test_cos, train_word_overlap, train_refuting, train_polarity, train_hand, \\\n",
        "  test_word_overlap, test_refuting, test_polarity, test_hand\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc414hkTQfgy"
      },
      "source": [
        "# Load Data (Embedding Matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OvsJxTJQQfg3"
      },
      "outputs": [],
      "source": [
        "# from util import *\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from random import randrange\n",
        "\n",
        "mode = 'train'\n",
        "\n",
        "# Set file names\n",
        "if local:\n",
        "    file_train_bodies = \"data/train_bodies.csv\"\n",
        "    file_train_instances = \"data/train_stances.csv\"\n",
        "    file_test_instances = \"data/competition_test_stances.csv\"\n",
        "    file_test_bodies = \"data/competition_test_bodies.csv\"\n",
        "    file_predictions = \"/predictions/NN_to_NN_predictions_test.csv\"\n",
        "else:\n",
        "    file_train_bodies = \"drive/MyDrive/4B/MSCI 598/Project/data/train_bodies.csv\"\n",
        "    file_train_instances = \"drive/MyDrive/4B/MSCI 598/Project/data/train_stances.csv\"\n",
        "    file_test_instances = \"drive/MyDrive/4B/MSCI 598/Project/data/competition_test_stances.csv\"\n",
        "    file_test_bodies = \"drive/MyDrive/4B/MSCI 598/Project/data/competition_test_bodies.csv\"\n",
        "    file_predictions = \"drive/MyDrive/4B/MSCI 598/Project/predictions/NN_to_NN_predictions_test.csv\"\n",
        "\n",
        "\n",
        "# Load data sets\n",
        "train_instances = read(file_train_instances)\n",
        "train_bodies = read(file_train_bodies)\n",
        "\n",
        "# print(train_instances[0]['Stance'] == 'disagree')\n",
        "\n",
        "train_disagree = [i for i in range(0, len(train_instances)) if train_instances[i]['Stance'] == 'disagree']\n",
        "train_agree = [i for i in range(0, len(train_instances)) if train_instances[i]['Stance'] == 'agree']\n",
        "# train_unrelated = [i for i in range(0, len(train_instances)) if train_instances[i]['Stance'] == 'unrelated']\n",
        "# train_discuss = [i for i in range(0, len(train_instances)) if train_instances[i]['Stance'] == 'discuss']\n",
        "\n",
        "# random.shuffle(train_unrelated)\n",
        "\n",
        "# print(len(train_instances), len(train_disagree), len(train_agree), len(train_unrelated), len(train_discuss))\n",
        "\n",
        "# train_stances_cleaned = train_disagree + train_agree + train_discuss\n",
        "# print(len(train_stances_cleaned))\n",
        "\n",
        "\n",
        "# counter = 0\n",
        "# while counter < 8909:\n",
        "#   train_stances_cleaned.append(train_instances[train_unrelated[counter]])\n",
        "#   counter += 1\n",
        "# print(len(train_stances_cleaned))\n",
        "\n",
        "counter = 0\n",
        "while counter < 8909-840:\n",
        "  # print(train_instances[train_disagree[randrange(0, len(train_disagree))]])\n",
        "  train_instances.append(train_instances[train_disagree[randrange(0, len(train_disagree))]])\n",
        "  # train_stances_cleaned.append(train_instances[train_disagree[randrange(0, len(train_disagree))]])\n",
        "  counter += 1\n",
        "\n",
        "counter = 0\n",
        "while counter < 8909-3678:\n",
        "  train_instances.append(train_instances[train_agree[randrange(0, len(train_agree))]])\n",
        "  # train_stances_cleaned.append(train_instances[train_agree[randrange(0, len(train_agree))]])\n",
        "  counter += 1\n",
        "\n",
        "# print(len(train_instances))\n",
        "# print(len(train_instances))\n",
        "\n",
        "\n",
        "# randomize = np.arange(len(train_instances))\n",
        "# np.random.shuffle(randomize)\n",
        "# print(randomize)\n",
        "# train_instances = train_instances[randomize]\n",
        "\n",
        "random.shuffle(train_instances)\n",
        "\n",
        "raw_train = FNCData(train_instances, train_bodies)\n",
        "raw_test = FNCData(read(file_test_instances), read(file_test_bodies))\n",
        "n_train = len(raw_train.instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1j38MywaS9C",
        "outputId": "dafb681e-da4f-4bef-b134-adc359d01798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads h test.instances len 25413 25413 25413\n",
            "heads h feat len 25413 25413 25413\n"
          ]
        }
      ],
      "source": [
        "# pre-processing\n",
        "embeddings_matrix, train_val_h, train_val_b, train_val_s,\\\n",
        " test_h, test_b, test_s, max_sent_len,\\\n",
        "  train_val_tf, train_val_cos, test_tf, test_cos, train_val_word_overlap, train_val_refuting, train_val_polarity, train_val_hand, \\\n",
        "  test_word_overlap, test_refuting, test_polarity, test_hand = preprocess(raw_train, raw_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOYm-S11hxtw",
        "outputId": "07626a41-7558-47c8-dd60-22d0ba5f59bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63272\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63272"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(len(train_val_h))\n",
        "len(train_val_word_overlap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zMG1d57sKHYl"
      },
      "outputs": [],
      "source": [
        "# splits\n",
        "train_h = []\n",
        "train_b = []\n",
        "train_s = []\n",
        "train_tf = []\n",
        "val_h = []\n",
        "val_b = []\n",
        "val_s = []\n",
        "val_tf = []\n",
        "val_cos = []\n",
        "\n",
        "train_cos = []\n",
        "train_word_overlap = []\n",
        "train_refuting = []\n",
        "train_polarity = []\n",
        "train_hand = []\n",
        "\n",
        "val_cos = []\n",
        "val_word_overlap = []\n",
        "val_refuting = []\n",
        "val_polarity = []\n",
        "val_hand = []\n",
        "\n",
        "train_feats = []\n",
        "val_feats = []\n",
        "test_feats = []\n",
        "\n",
        "train_set_len = int(len(train_val_h)*0.9)\n",
        "feature_size = len(train_val_tf[0])\n",
        "\n",
        "for i in range(0, len(train_val_h)):\n",
        "  if i <= train_set_len:\n",
        "    train_h.append(train_val_h[i])\n",
        "    train_b.append(train_val_b[i])\n",
        "    train_s.append(train_val_s[i])\n",
        "    train_tf.append(train_val_tf[i])\n",
        "    train_cos.append(train_val_cos[i])\n",
        "    train_word_overlap.append(train_val_word_overlap[i])\n",
        "    train_refuting.append(train_val_refuting[i])\n",
        "    train_polarity.append(train_val_polarity[i])\n",
        "    train_hand.append(train_val_hand[i])\n",
        "    # train_feats.append([train_val_word_overlap[i], train_val_refuting[i], train_val_polarity[i], train_val_hand[i], train_val_cos[i]])\n",
        "  else:\n",
        "    val_h.append(train_val_h[i])\n",
        "    val_b.append(train_val_b[i])\n",
        "    val_s.append(train_val_s[i])\n",
        "    val_tf.append(train_val_tf[i])\n",
        "    val_cos.append(train_val_cos[i])\n",
        "    val_word_overlap.append(train_val_word_overlap[i])\n",
        "    val_refuting.append(train_val_refuting[i])\n",
        "    val_polarity.append(train_val_polarity[i])\n",
        "    val_hand.append(train_val_hand[i])\n",
        "    # val_feats.append([train_val_word_overlap[i], train_val_refuting[i], train_val_polarity[i], train_val_hand[i], train_val_cos[i]])\n",
        "\n",
        "# for i in range(0, len(test_h)):\n",
        "  # test_feats.append([test_word_overlap[i], test_refuting[i], test_polarity[i], test_hand[i], test_cos[i]])\n",
        "\n",
        "\n",
        "train_h = np.array(train_h)\n",
        "train_b = np.array(train_b)\n",
        "train_s = np.array(train_s)\n",
        "train_tf = np.array(train_tf)\n",
        "train_cos = np.array(train_cos)\n",
        "# train_word_overlap = np.array(train_word_overlap)\n",
        "# train_refuting = np.array(train_refuting)\n",
        "# train_polarity = np.array(train_polarity)\n",
        "# train_hand = np.array(train_hand)\n",
        "\n",
        "val_h = np.array(val_h)\n",
        "val_b = np.array(val_b)\n",
        "val_s = np.array(val_s)\n",
        "val_tf = np.array(val_tf)\n",
        "val_cos = np.array(val_cos)\n",
        "# val_word_overlap = np.array(val_word_overlap)\n",
        "# val_refuting = np.array(val_refuting)\n",
        "# val_polarity = np.array(val_polarity)\n",
        "# val_hand = np.array(val_hand)\n",
        "\n",
        "test_h = np.array(test_h)\n",
        "test_b = np.array(test_b)\n",
        "test_s = np.array(test_s)\n",
        "test_tf = np.array(test_tf)\n",
        "test_cos = np.array(test_cos)\n",
        "# test_word_overlap = np.array(test_word_overlap)\n",
        "# test_refuting = np.array(test_refuting)\n",
        "# test_polarity = np.array(test_polarity)\n",
        "# test_hand = np.array(test_hand)\n",
        "\n",
        "train_feats = np.c_[train_word_overlap, train_refuting, train_polarity, train_hand]\n",
        "val_feats = np.c_[val_word_overlap, val_refuting, val_polarity, val_hand]\n",
        "test_feats = np.c_[test_word_overlap, test_refuting, test_polarity, test_hand]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B8CGdiBBB7k",
        "outputId": "d648af9a-7a78-49d9-fae5-1ed1c8f7fdd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25413\n",
            "25413\n",
            "56945\n",
            "56945\n"
          ]
        }
      ],
      "source": [
        "print(len(test_feats))\n",
        "print(len(test_h))\n",
        "\n",
        "print(len(train_h))\n",
        "print(len(train_feats))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "-7oEzw0dE9db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "#ref\n",
        "label_ref_keras = {'agree': [1,0], 'disagree': [1,0], 'discuss': [1,0], 'unrelated': [0,1]}\n",
        "\n",
        "model = keras.models.load_model('drive/MyDrive/4B/MSCI 598/Project/models/' + 'rel_unrel_cnn_to_nn_8909x3_36545x1')\n",
        "y_pro = model.predict([test_h, test_b, test_tf, test_cos, test_feats])\n",
        "y_classes = y_pro.argmax(axis=-1)\n",
        "\n",
        "\n",
        "test_h2 = []\n",
        "test_b2 = []\n",
        "test_tf2 = []\n",
        "test_cos2 = []\n",
        "test_feats2 = []\n",
        "\n",
        "\n",
        "related_ind_to_ind = {}\n",
        "for i in range(0, len(y_classes)):\n",
        "  if y_classes[i] == 0:\n",
        "    test_h2.append(test_h[i])\n",
        "    test_b2.append(test_b[i])\n",
        "    test_tf2.append(test_tf[i])\n",
        "    test_cos2.append(test_cos[i])\n",
        "    test_feats2.append(test_feats[i])\n",
        "    related_ind_to_ind[len(test_h2)-1] = i\n",
        "  else:\n",
        "    y_classes[i] = 3\n",
        "\n",
        "\n",
        "# print(y_pro)\n",
        "# print(y_classes)"
      ],
      "metadata": {
        "id": "qbcV_iyPEsci"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ref\n",
        "label_ref_keras = {'agree': [1,0,0], 'disagree': [0,1,0], 'discuss': [0,0,1]}\n",
        "label_ref_keras = {'agree': [1,0,0,0], 'disagree': [0,1,0,0], 'discuss': [0,0,1,0], 'unrelated': [0,0,0,1]}\n",
        "\n",
        "# model2 = keras.models.load_model('drive/MyDrive/4B/MSCI 598/Project/models/' + '3_classes_cnn_to_nn_8909x3_36545x1')\n",
        "# model2 = keras.models.load_model('drive/MyDrive/4B/MSCI 598/Project/models/' + '3_classes_cnn_to_nn_8909x3_36545x1_lr=0.001')\n",
        "model2 = keras.models.load_model('drive/MyDrive/4B/MSCI 598/Project/models/' + '3_classes_cnn_to_nn_8909x3_36545x1_lr=0.0005_tmp')\n",
        "y_pro2 = model2.predict([np.array(test_h2), np.array(test_b2), np.array(test_tf2), np.array(test_cos2), np.array(test_feats2)])\n",
        "y_classes2 = y_pro2.argmax(axis=-1)\n",
        "\n",
        "for i in range(0, len(y_classes2)):\n",
        "  y_classes[related_ind_to_ind[i]] = y_classes2[i]\n",
        "  \n"
      ],
      "metadata": {
        "id": "MGwd7jyDJryq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058cbd4c-a9fb-413f-c966-6f06db2b1f56",
        "id": "3WjV46tHGni1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.38      0.44      1903\n",
            "           1       0.30      0.16      0.21       697\n",
            "           2       0.72      0.79      0.75      4464\n",
            "           3       0.97      0.99      0.98     18349\n",
            "\n",
            "    accuracy                           0.88     25413\n",
            "   macro avg       0.63      0.58      0.59     25413\n",
            "weighted avg       0.87      0.88      0.88     25413\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_s_class_num = []\n",
        "for t in test_s:\n",
        "  cur = 0\n",
        "  if t[0] == 1:\n",
        "    cur = 0\n",
        "  elif t[1] == 1:\n",
        "    cur = 1\n",
        "  elif t[2] == 1:\n",
        "    cur = 2\n",
        "  elif t[3] == 1:\n",
        "    cur = 3\n",
        "  test_s_class_num.append(cur)\n",
        "\n",
        "print(classification_report(test_s_class_num, y_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1063a79-e702-4127-b881-bcbf78ef2e69",
        "id": "KiqEOVMMGni2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |    723    |    127    |    896    |    157    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |    177    |    113    |    280    |    127    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |    480    |    122    |   3508    |    354    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |    35     |     9     |    176    |   18129   |\n",
            "-------------------------------------------------------------\n",
            "Score: 9396.75 out of 11651.25\t(80.65014483424525%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80.65014483424525"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "LABELS_RELATED = ['unrelated','related']\n",
        "RELATED = LABELS[0:3]\n",
        "\n",
        "\n",
        "predicted = [LABELS[a] for a in y_classes]\n",
        "actual =[LABELS[a] for a in test_s_class_num]\n",
        "report_score(actual, predicted)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save final answer\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(np.column_stack([[instance[\"Headline\"] for instance in raw_test.instances], [instance[\"Body ID\"] for instance in raw_test.instances], predicted]), \n",
        "                            columns=['Headline', 'Body ID', 'Stance'])\n",
        "df.to_csv('drive/MyDrive/4B/MSCI 598/Project/results/answer.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "c_unJPbjGni3"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Inference_final_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}